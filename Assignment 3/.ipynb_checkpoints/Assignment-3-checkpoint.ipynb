{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMSC478 Machine Learning - Spring 2020\n",
    "\n",
    "## Instructor: Fereydoon Vafaei\n",
    "\n",
    "### <font color=\"blue\">Assignment-3: Dimensionality Reduction with PCA and Image Segmentation with K-Means and GMM</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ching Zheng - JA93772"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Assignment-3, you're going to perform dimensionality reduction using PCA, as well as image segmentation using K-Means and GMM.\n",
    "\n",
    "Pedagogically, this assignment will help you:\n",
    "- practice PCA.\n",
    "- pratice K-Means and GMM using scikit-learn.\n",
    "\n",
    "To help you better understand how image segmentation is performed in scikit-learn, Part II image segmentation is provided more similar to a tutorial than an assignment. You still need to complete Part II and generate all the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I - PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the [data](https://github.com/fereydoonvafaei/CMSC478-Spring2020/blob/master/Assignment-3/diabetes.csv). You can read the descriptions of data [here](https://www.kaggle.com/uciml/pima-indians-diabetes-database)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diabetes data with Pandas\n",
    "df = pd.read_csv(\"diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8)\n",
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "# Create X, y - Outcome is the target\n",
    "X = df.drop(\"Outcome\", axis=1)\n",
    "y = df[\"Outcome\"]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to train and test with a ratio of 0.3 for test set\n",
    "...\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You may need to fine-tune your model to get at least 0.82 for mean auc score in cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC Score - Random Forest:  0.8334472934472934\n"
     ]
    }
   ],
   "source": [
    "# Perform a 10-fold cross validation with 'roc_auc' as scoring - CV should be on the whole dataset\n",
    "rf_cv_score = cross_val_score(rf_clf, X, y, cv = 10, scoring = \"roc_auc\")\n",
    "# print the cv_score mean\n",
    "print(\"Mean AUC Score - Random Forest: \", rf_cv_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can now use `feature_importances_` to see the importance of each feature in the random forest classifier. As mentioned in the lecture, it is measured based on how much the tree nodes that use that feature reduce impurity on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregnancies 0.07893645665682904\n",
      "Glucose 0.2502985824698355\n",
      "BloodPressure 0.09878746880926602\n",
      "SkinThickness 0.07059733922668819\n",
      "Insulin 0.06896841239738276\n",
      "BMI 0.173019652572881\n",
      "DiabetesPedigreeFunction 0.1250300415525202\n",
      "Age 0.1343620463145973\n"
     ]
    }
   ],
   "source": [
    "# Fit the rf_clf on the training set\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature_importances for all features using random forest classifier\n",
    "for x, y in zip(df, rf_clf.feature_importances_):\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, plot the ROC curve of random forest. Plot should be complete (including title, axis labels, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'False Positive Rate')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot the complete ROC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "rf_probs = rf_clf.predict_proba(X_test)\n",
    "rf_preds = rf_probs[:,1]\n",
    "rf_fpr, rf_tpr, rf_threshold = metrics.roc_curve(y_test, rf_preds)\n",
    "rf_roc_auc = metrics.auc(rf_fpr, rf_tpr)\n",
    "\n",
    "plt.title('ROC')\n",
    "plt.plot(rf_fpr, rf_tpr, 'b', label = 'AUC_RF = %0.2f' % rf_roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next, it's time to do dimensionality reduction using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 4)\n"
     ]
    }
   ],
   "source": [
    "# Create a pca with 4 principal components, and apply it on X and store the transformed data in X4D\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 4)\n",
    "X4D = pca.fit_transform(X)\n",
    "print(X4D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-75.71465491, -35.95078264,  -7.26078895,  15.66926931])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X4D[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.02176587e-03,  9.78115765e-02,  1.60930503e-02,  6.07566861e-02,\n",
       "        9.93110844e-01,  1.40108085e-02,  5.37167919e-04, -3.56474430e-03])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the pca components\n",
    "pca.components_.T[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(514, 4)\n",
      "(537,)\n",
      "(254, 4)\n",
      "(231,)\n"
     ]
    }
   ],
   "source": [
    "# Split the X4D to train and test\n",
    "X4D_train, X4D_test = train_test_split(X4D, test_size=0.33)\n",
    "\n",
    "print(X4D_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X4D_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">You can see the coefficients of each Principal Component (PC) corresponding to each feature as each PC is a linear combination of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>PC-1</td>\n",
       "      <td>-0.002022</td>\n",
       "      <td>0.097812</td>\n",
       "      <td>0.016093</td>\n",
       "      <td>0.060757</td>\n",
       "      <td>0.993111</td>\n",
       "      <td>0.014011</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>-0.003565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>PC-2</td>\n",
       "      <td>-0.022649</td>\n",
       "      <td>-0.972210</td>\n",
       "      <td>-0.141909</td>\n",
       "      <td>0.057861</td>\n",
       "      <td>0.094627</td>\n",
       "      <td>-0.046973</td>\n",
       "      <td>-0.000817</td>\n",
       "      <td>-0.140168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>PC-3</td>\n",
       "      <td>-0.022465</td>\n",
       "      <td>0.143429</td>\n",
       "      <td>-0.922467</td>\n",
       "      <td>-0.307013</td>\n",
       "      <td>0.020977</td>\n",
       "      <td>-0.132445</td>\n",
       "      <td>-0.000640</td>\n",
       "      <td>-0.125454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>PC-4</td>\n",
       "      <td>-0.049046</td>\n",
       "      <td>0.119830</td>\n",
       "      <td>-0.262743</td>\n",
       "      <td>0.884369</td>\n",
       "      <td>-0.065550</td>\n",
       "      <td>0.192802</td>\n",
       "      <td>0.002699</td>\n",
       "      <td>-0.301024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Pregnancies   Glucose  BloodPressure  SkinThickness   Insulin       BMI  \\\n",
       "PC-1    -0.002022  0.097812       0.016093       0.060757  0.993111  0.014011   \n",
       "PC-2    -0.022649 -0.972210      -0.141909       0.057861  0.094627 -0.046973   \n",
       "PC-3    -0.022465  0.143429      -0.922467      -0.307013  0.020977 -0.132445   \n",
       "PC-4    -0.049046  0.119830      -0.262743       0.884369 -0.065550  0.192802   \n",
       "\n",
       "      DiabetesPedigreeFunction       Age  \n",
       "PC-1                  0.000537 -0.003565  \n",
       "PC-2                 -0.000817 -0.140168  \n",
       "PC-3                 -0.000640 -0.125454  \n",
       "PC-4                  0.002699 -0.301024  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pca.components_,columns=X.columns,index = ['PC-1','PC-2', 'PC-3', 'PC-4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">As discussed in the lecture, once you get your X4D, you can see the `explained_variance_ratio_` which shows the ratio of variance preserved in each Principal Component (PC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88854663, 0.06159078, 0.02579012, 0.01308614])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I - Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER THE FOLLOWING QUESTIONS HERE:\n",
    "\n",
    "**Q1 [10 points]** - Specify for each PC, which feature has the highest correlation with the corresponding PC? You should name the feature that has the highest coefficient in the dataframe above - the one which has PCs as rows.\n",
    "\n",
    "Answer: For PC-1 the highest correlation is insulin with a score of 0.99. For PC-2 the highest correlation is also insulin with a score of 0.094627. For PC-3 the highest correlation is insulin with a score of 0.020977. For PC-4 the highest correlation is BMI with a score of 0.192802.\n",
    "\n",
    "**Q2 [10 points]** - Which of the 8 features has the highest importance in random forest `feature_importances_` which also has a high correlation with one of the 4 PCs returned by PCA? Notice that this can be an indication of consistency between PCA results and `feature_importances_` of rf classifier.\n",
    "\n",
    "Answer: The Glucose has the highest importance in the random forest classifier with a score of  0.2286; thus it would correlate with PC-4 the most since that is its highest correlation with the other PCs. \n",
    "\n",
    "**Q3 [20 points]** - Compare the mean cv score (using roc_auc as scoring) of random forest classifier using only 1 principal component, versus 2, 3, and 4 prinicipal compenents. Use the cell below to write all necessary code to answer this question. Compare your results with `explained_variance_ratio_` and infer the optimal number of principal components based on your comaprison. GIVE COMPLETE ANSWER WITH ARGUMENTS AND REASONING! No partial credit for incomplete answers!\n",
    "\n",
    "Answer: \n",
    "\n",
    "Cross-Validation     PCs   Variance Ratio   \n",
    "0.7728433048433049 : X4D : 0.01308614\n",
    "0.7603176638176637 : X3D : 0.02579012\n",
    "0.7537393162393162 : X2D : 0.06159078\n",
    "0.6567079772079772 : X1D : 0.88854663\n",
    "\n",
    "Based on the output we can infer that the optimal number of principal components is 2 given that the variance ratio for PC-2 is 0.06159078 which means that 6.259% % of the dataset's variance lies along the second PC. Furthurmore, the cross validation score between the X2D and X4D only has a difference of ~0.2. So depsite having a significantly lower amount of variance, X2D still has a cross validation score relatively close to the highest cross validation score of .7728. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array 0.1343620463145973 cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-60243d3cdc07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mrf_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mrf_cv_score_4XD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX4D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"roc_auc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf_cv_score_X4D\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    389\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    392\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     \"\"\"\n\u001b[1;32m--> 217\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \"\"\"\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \"\"\"\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[1;32m--> 146\u001b[1;33m                             \" a valid collection.\" % x)\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;31m# Dask dataframes may not return numeric shape[0] value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Singleton array 0.1343620463145973 cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "# Write all the necessary code to answer Q3 on comparing different number of PCs (1 to 4).\n",
    "# You should compare based on mean cv score using 'roc_auc' as scoring and 10 fold cv\n",
    "\n",
    "# pca = PCA(n_components = 4)\n",
    "# X4D = pca.fit_transform(X)\n",
    "# rf_clf = RandomForestClassifier(n_estimators = 100)\n",
    "# rf_cv_score = cross_val_score(rf_clf, X, y, cv = 10, scoring = \"roc_auc\")\n",
    "\n",
    "# pca1 = PCA(n_components = 1)\n",
    "# X1D = pca1.fit_transform(X)\n",
    "# rf_clf_X1D = RandomForestClassifier(n_estimators = 100)\n",
    "# rf_cv_score_X1D = cross_val_score(rf_clf_X1D, X1D, y, cv = 10, scoring = \"roc_auc\")\n",
    "\n",
    "# pca2 = PCA(n_components = 2)\n",
    "# X2D = pc2.fit_transform(X)\n",
    "# rf_clf_X2D = RandomForestClassifier(n_estimators = 100)\n",
    "# rf_cv_score_X2D = cross_val_score(rf_clf_X2D, X2D, y, cv = 10, scoring = \"roc_auc\")\n",
    "\n",
    "# pca3 = PCA(n_components = 3)\n",
    "# X3D = pca3.fit_transform(X)\n",
    "# rf_clf_X3D = RandomForestClassifier(n_estimators = 100)\n",
    "# rf_cv_score_X3D = cross_val_score(rf_clf_X3D, X3D, y, cv = 10, scoring = \"roc_auc\")\n",
    "\n",
    "# pca4 = PCA(n_components = 4)\n",
    "# X4D = pca4.fit_transform(X)\n",
    "# rf_clf_X4D = RandomForestClassifier(n_estimators = 100)\n",
    "# rf_cv_score_X4D = cross_val_score(rf_clf_X4D, X4D, y, cv = 10, scoring = \"roc_auc\")\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators = 100)\n",
    "rf_cv_score_4XD = cross_val_score(rf_clf, X4D, y, cv = 10, scoring = \"roc_auc\")\n",
    "\n",
    "print(rf_cv_score_X4D.mean())\n",
    "print(rf_cv_score_X3D.mean())\n",
    "print(rf_cv_score_X2D.mean())\n",
    "print(rf_cv_score_X1D.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II - Image Segmentation with K-Means and GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part II, you're going to perform image segmentation using K-Means algorithm and Gaussian Mixture Models (GMM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means was fully explained in the lectures. To use GMM for clustering, one of the main approaches is EM algorithm explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Expectation Maximization (EM) algorithm attempts estimating the weights $\\phi$ and all the distribution parameters $\\mu^{(1)}$ to $\\mu^{(k)}$ and $\\sum^{(1)}$ to $\\sum^{(k)}$ given the dataset X.\n",
    "\n",
    "\n",
    "- EM has many similarities with the K-Means algorithm. It initializes the cluster parameters randomly, then it repeats two steps until convergence:\n",
    "    - First, assigning instances to clusters - this is called the **expectation step**.\n",
    "    - Then, updating the clusters - this is called the **maximization step**.\n",
    "    \n",
    "    \n",
    "- In the context of clustering, you can think of EM as a generalization of K-Means that not only finds the cluster centers $\\mu^{(1)}$ to $\\mu^{(k)}$, but also their size, shape, and orientation $\\sum^{(1)}$ to $\\sum^{(k)}$​, as well as their relative weights $\\phi^{(1)}$ to $\\phi^{(k)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unlike K-Means, EM uses **soft** cluster assignments, not **hard** assignments.\n",
    "\n",
    "\n",
    "- For each instance, during the **expectation step**, the algorithm estimates the probability that it belongs to each cluster (based on the current cluster parameters).\n",
    "\n",
    "\n",
    "- Then, during the **maximization step**, each cluster is updated using all the instances in the dataset, with each instance weighted by the estimated probability that it belongs to that cluster.\n",
    "\n",
    "\n",
    "- These probabilities are called the **responsibilities** of the clusters for the instances.\n",
    "\n",
    "\n",
    "- During the **maximization step**, each cluster’s update will mostly be impacted by the instances it is most **responsible** for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the lectures, there are different applications for clustering. One application is image segmentation which you are going to do on a simple image as practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are going to use the implementations of K-Means and GMM provided in scikit-learn. Let's see one simple example first with K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Download [pic.jpeg](https://github.com/fereydoonvafaei/CMSC478-Spring2020/blob/master/Assignment-3/pic.jpeg) and save it in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pic - Don't forget to include matplotlib in the 1st cell at the top of the notebook!\n",
    "pic = plt.imread('pic.jpeg')/255  # dividing by 255 to bring the pixel values between 0 and 1\n",
    "print(pic.shape)\n",
    "plt.imshow(pic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the pic to make it a 2D array\n",
    "pic_array = pic.reshape(pic.shape[0]*pic.shape[1], pic.shape[2])\n",
    "pic_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import K-Means\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's try with 5 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 clusters\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(pic_array)\n",
    "pic2show = kmeans.cluster_centers_[kmeans.labels_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_pic = pic2show.reshape(pic.shape[0], pic.shape[1], pic.shape[2])\n",
    "plt.imshow(cluster_pic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Looks like a reasonable number of segments/clusters as you have the sky, clouds, trees, lake and the ground. Try with different number of clusters/segments and see the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now download [cat.jpeg](https://github.com/fereydoonvafaei/CMSC478-Spring2020/blob/master/Assignment-3/cat.jpeg) and save it in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = plt.imread('cat.jpeg')/255  # dividing by 255 to bring the pixel values between 0 and 1\n",
    "print(cat.shape)\n",
    "plt.imshow(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape cat to make it a 2D array \n",
    "\n",
    "cat_array = cat.reshape(cat.shape[0]*cat.shape[1], cat.shape[2])\n",
    "cat_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means algorithms on the cat image with 5 clusters/segments,\n",
    "# and display the segmented image\n",
    "kmeans2 = KMeans(n_clusters=5, random_state=0).fit(cat_array)\n",
    "cat2show = kmeans2.cluster_centers_[kmeans2.labels_]\n",
    "\n",
    "cluster_pic2 = cat2show.reshape(cat.shape[0], cat.shape[1], cat.shape[2])\n",
    "plt.imshow(cluster_pic2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now let's use [GMM from scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a gm model with 5 clusters\n",
    "# you may check out the textbook or scikit-learn documentation to see how you can use .fit() method.\n",
    "gm = GaussianMixture(n_components=5, n_init=10)\n",
    "gm.fit(cat_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.converged_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_clusters = gm.predict(cat_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_clusters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now there are two ways to display the segments, both provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_seg_gm = gm_clusters.reshape(cat.shape[0], cat.shape[1])\n",
    "print(cat_seg_gm.shape)\n",
    "plt.imshow(cat_seg_gm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gm.means_[cat_seg_gm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Looks like GMM actually did a better job on segmenting the background (blue carpet) although not so good of a visuall appearance for the cat itself. You can now try with some other images of your choice and have some fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "Assignment-3 has a maximum of 100 points. Make sure that you get the correct outputs for all cells that you implement and give complete answers to all questions. Also, your notebook should be written with no grammatical and spelling errors and should be easy-to-read.\n",
    "\n",
    "The breakdown of the 100 points is as follows:\n",
    "\n",
    "- Part I PCA: 80 points\n",
    "    - Implementation: 40 points\n",
    "    - Questions: 40 points\n",
    "\n",
    "- Part II Image Segmentation using K-Means & GMM - 20 points\n",
    "    - K-Means: 10 points\n",
    "    - GMM: 10 points\n",
    "   \n",
    "\n",
    "<b>Note: </b>Follow the instructions of each section carefully. Up to 10 points may be deducted if your submitted notebook is not easy to read and follow or if it has grammatical, spelling or formatting issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name your notebook ```Lastname-A3.ipynb```. Submit the completed notebook using the ```Assignment-3``` link on Blackboard.\n",
    "\n",
    "Grading will be based on \n",
    "\n",
    "  * correct implementation, correct answer to the questions, and\n",
    "  * readability of the notebook.\n",
    "  \n",
    "<font color=red><b>Due Date: Thursday March 23, 11:59PM.</b></font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
